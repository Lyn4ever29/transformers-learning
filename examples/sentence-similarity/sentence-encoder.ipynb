{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 训练一个向量编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T06:43:35.453613Z",
     "start_time": "2023-09-07T06:43:31.713657300Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,TrainingArguments,Trainer,DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 1. 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T06:43:57.401818700Z",
     "start_time": "2023-09-07T06:43:35.082732400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "datas = load_dataset(\"shibing624/sts-sohu2021\",'dda')\n",
    "# datas.save_to_disk(\"./data/sts-sohu2021/dda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T06:43:57.402817100Z",
     "start_time": "2023-09-07T06:43:40.875061Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['sentence1', 'sentence2', 'label'],\n",
       "         num_rows: 10512\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['sentence1', 'sentence2', 'label'],\n",
       "         num_rows: 1000\n",
       "     })\n",
       " }),\n",
       " {'sentence1': '高晓松瘦身成功减肥30斤，被路人认成吴亦凡！',\n",
       "  'sentence2': '景甜亮因穿着成“焦点”，看到最后一张，网友：美的有点过分了！',\n",
       "  'label': 0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据预览\n",
    "datas,datas['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T06:43:57.402817100Z",
     "start_time": "2023-09-07T06:43:40.879051500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 定义数据处理函数\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
    "\n",
    "def process_fun(examples):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for sen1, sen2, label in zip(examples[\"sentence1\"], examples[\"sentence2\"], examples[\"label\"]):\n",
    "        sentences.append(sen1)\n",
    "        sentences.append(sen2)\n",
    "        labels.append(1 if int(label) == 1 else -1)\n",
    "    # input_ids, attention_mask, token_type_ids\n",
    "    tokenized_examples = tokenizer(sentences, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    tokenized_examples = {k: [v[i: i + 2] for i in range(0, len(v), 2)] for k, v in tokenized_examples.items()}\n",
    "    tokenized_examples[\"labels\"] = labels\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T06:43:57.402817100Z",
     "start_time": "2023-09-07T06:43:40.880049Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd16ca7087694bb59ec6ad5fba9e6593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc08936d898340cfb6a2c93a0a6e8c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_tokenizer = datas.map(process_fun,batched=True,remove_columns=datas[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 2. 加载模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T06:43:57.403813800Z",
     "start_time": "2023-09-07T06:43:40.883039800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification,BertModel\n",
    "# 导入余弦函数\n",
    "from torch.nn import CosineSimilarity,CosineEmbeddingLoss\n",
    "\n",
    "from typing import Optional\n",
    "import torch\n",
    "class SentenceEncoderModel(BertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "            super().__init__(config)\n",
    "            self.num_labels = config.num_labels\n",
    "            self.config = config\n",
    "            self.bert = BertModel(config)\n",
    "            # Initialize weights and apply final processing\n",
    "            self.post_init()\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        # 获取sentenceA 和 sentenceB的输入\n",
    "        senA_input_ids, senB_input_ids = input_ids[:, 0], input_ids[:, 1]\n",
    "        senA_attention_mask, senB_attention_mask = attention_mask[:, 0], attention_mask[:, 1]\n",
    "        senA_token_type_ids, senB_token_type_ids = token_type_ids[:, 0], token_type_ids[:, 1]\n",
    "\n",
    "        # 分别获取sentenceA 和 sentenceB的向量表示\n",
    "        senA_outputs = self.bert(\n",
    "            senA_input_ids,\n",
    "            attention_mask=senA_attention_mask,\n",
    "            token_type_ids=senA_token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        senA_pooled_output = senA_outputs[1]    # [batch, hidden]\n",
    "\n",
    "        senB_outputs = self.bert(\n",
    "            senB_input_ids,\n",
    "            attention_mask=senB_attention_mask,\n",
    "            token_type_ids=senB_token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        senB_pooled_output = senB_outputs[1]    # [batch, hidden]\n",
    "\n",
    "        # 计算相似度\n",
    "\n",
    "        cos = CosineSimilarity()(senA_pooled_output, senB_pooled_output)    # [batch, ]\n",
    "\n",
    "        # 计算loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CosineEmbeddingLoss(0.3)\n",
    "            loss = loss_fct(senA_pooled_output, senB_pooled_output, labels)\n",
    "\n",
    "        output = (cos,)\n",
    "        return ((loss,) + output) if loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SentenceEncoderModel were not initialized from the model checkpoint at /data1/model/chinese-macbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SentenceEncoderModel.from_pretrained(\"hfl/chinese-macbert-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 3.创建评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T06:43:57.403813800Z",
     "start_time": "2023-09-07T06:43:42.247085300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f1_metirc = evaluate.load(\"f1\")\n",
    "\n",
    "def eval_metric(eval_predict):\n",
    "    predictions, labels = eval_predict\n",
    "    # 这里需要一个置信度，代表概率大于0.7的我们就认为i相似\n",
    "    predictions = [int(p > 0.7) for p in predictions]\n",
    "    labels = [int(l > 0) for l in labels]\n",
    "    # predictions = predictions.argmax(axis=-1)\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metirc.compute(predictions=predictions, references=labels)\n",
    "    acc.update(f1)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 4. 设置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T06:43:57.403813800Z",
     "start_time": "2023-09-07T06:43:52.840378100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(output_dir=\"./encoder_model\",      # 输出文件夹\n",
    "                               per_device_train_batch_size=32,  # 训练时的batch_size\n",
    "                               per_device_eval_batch_size=32,  # 验证时的batch_size\n",
    "                               logging_steps=10,                # log 打印的频率\n",
    "                               evaluation_strategy=\"epoch\",     # 评估策略\n",
    "                               save_strategy=\"epoch\",           # 保存策略\n",
    "                               save_total_limit=3,              # 最大保存数\n",
    "                               learning_rate=2e-5,              # 学习率\n",
    "                               weight_decay=0.01,               # weight_decay\n",
    "                               metric_for_best_model=\"f1\",      # 设定评估指标\n",
    "                               load_best_model_at_end=True)     # 训练完成后加载最优模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 5. 定义训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T06:43:57.404810700Z",
     "start_time": "2023-09-07T06:43:52.840378100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=train_args,\n",
    "                  train_dataset=data_tokenizer[\"train\"],\n",
    "                  eval_dataset=data_tokenizer[\"test\"],\n",
    "                  data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "                  compute_metrics=eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 6. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-07T06:43:52.840378100Z"
    },
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='987' max='987' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [987/987 12:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.210900</td>\n",
       "      <td>0.186134</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.578616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.183200</td>\n",
       "      <td>0.167410</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.630928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.162300</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.640816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=987, training_loss=0.1817621789200811, metrics={'train_runtime': 769.2546, 'train_samples_per_second': 40.996, 'train_steps_per_second': 1.283, 'total_flos': 4148735120916480.0, 'train_loss': 0.1817621789200811, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 7. 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.16230031847953796,\n",
       " 'eval_accuracy': 0.824,\n",
       " 'eval_f1': 0.6408163265306122,\n",
       " 'eval_runtime': 7.8649,\n",
       " 'eval_samples_per_second': 127.148,\n",
       " 'eval_steps_per_second': 4.069,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result = trainer.evaluate(data_tokenizer[\"test\"])\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 8.推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2769, 1599, 3614, 1266,  776,  102,    0,    0],\n",
       "         [ 101,  791, 1921, 1921, 3698, 2582,  720, 3416,  102]],\n",
       "        device='cuda:0'),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由于是自定义模型，这里就要自己写推理方法了,就是利用模型对输入的数据进行编码，然后手动计算相似度\n",
    "text1=\"我喜欢北京\"\n",
    "text2=\"今天天气怎么样\"\n",
    "inputs  = tokenizer([text1, text2], max_length=128, truncation=True, return_tensors=\"pt\", padding=True)\n",
    "inputs = {k: v.to('cuda:0') for k, v in inputs.items()}\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2078535109758377"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用这个编码模型，编码两个句子后，计算这两个句子是否相似\n",
    "output = model.bert(**inputs)\n",
    "logits=output[1] # 2*768\n",
    "cos = CosineSimilarity()(logits[None, 0, :], logits[None,1, :]).squeeze().cpu().item()\n",
    "cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "不相似"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('相似' if cos>0.7 else '不相似')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
