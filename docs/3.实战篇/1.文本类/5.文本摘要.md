## 任务描述
文本摘要是指通过各种技术，对文本或者是文本的集合，抽取、总结或是精炼其中的要点信息，用以概括和展示原始文本的主要内容或大意。作为文本生成任务的主要方向之一，从本质上而言，这是一种信息压缩技术。文本摘要技术是信息爆炸时代，提升人们获取有效信息效率的关键技术之一，如何从冗余、非结构化的长文本中提炼出关键信息，构成精简通顺的摘要，是文本摘要的核心问题。
## 实验概述
在之前的文章[6.预训练模型实战](6.预训练模型实战.md)中，介绍到了四种预训练语言模型，四种语言模型都可以做文本摘要任务，但像bert这种掩码语言模型效果就相对差一点儿。本文选取四种语言模型，在文本摘要任务上做对比试验。
## 实验配置
- 数据集
[LCSTS_new中文短摘要生成数据集](https://www.luge.ai/#/luge/dataDetail?id=10)
2015年发布的生成式短摘要数据集，以微博原文为输入，1~2句话的短摘要为输出。
- 预训练模型
1. chinese-macbert-base
2. mengzi-t5-base
3. bloom-560m
4. 